{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7abc111a-5b1a-49b3-b760-edb439f04e76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\n",
      "Se comienza a scrapear: Nuda propiedad\u001b[0m\n",
      "\n",
      "Scrapeando el tipo de situación: Nuda propiedad en la URL: https://www.idealista.com/venta-viviendas/cordoba-cordoba/con-nuda-propiedad/\n",
      " Hay 18 anuncios en la página 1\n",
      "\n",
      "Scrapeando el tipo de situación: Nuda propiedad en la URL: https://www.idealista.com/venta-viviendas/cordoba-cordoba/con-nuda-propiedad/pagina-2.htm\n",
      "La página no existe y se ha redirigido a la 1, scrapeo de la situación: Nuda propiedad ha finalizado\n",
      "\n",
      " Datos guardados en idealista_resultados1.csv y idealista_resultados1.json\n",
      "\n",
      " Tiempo total de ejecución: 739.89 segundos\n"
     ]
    }
   ],
   "source": [
    "# Librerías necesarias\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import random\n",
    "import re\n",
    "from pymongo import MongoClient\n",
    "\n",
    "# Tiempo al inicio de ejecutar el código: para saber el tiempo total de ejecucción\n",
    "inicio = time.time()\n",
    "\n",
    "# URL de idealista sin filtrar por ciudad\n",
    "base_url = 'https://www.idealista.com'\n",
    "\n",
    "# Encabezados para simular un navegador real y evitar bloqueos\n",
    "headers_list = [\n",
    "    {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/123.0.0.0 Safari/537.36',\n",
    "        'Accept-Language': 'es-ES,es;q=0.9',\n",
    "        'Referer': 'https://www.google.com/'\n",
    "    },\n",
    "    {\n",
    "        'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64; rv:115.0) Gecko/20100101 Firefox/115.0',\n",
    "        'Accept-Language': 'es-ES,es;q=0.8',\n",
    "        'Referer': 'https://www.bing.com/'\n",
    "    },\n",
    "    {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.6261.94 Safari/537.36 Edg/122.0.0.0',\n",
    "        'Accept-Language': 'es-ES,es;q=0.7',\n",
    "        'Referer': 'https://duckduckgo.com/'\n",
    "    },\n",
    "    {\n",
    "        'User-Agent': 'Mozilla/5.0 (Linux; Android 10; SM-G970F) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Mobile Safari/537.36',\n",
    "        'Accept-Language': 'es-ES,es;q=0.9',\n",
    "        'Referer': 'https://www.google.com/'\n",
    "    },\n",
    "    {\n",
    "        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_3) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.0 Safari/605.1.15',\n",
    "        'Accept-Language': 'es-ES,es;q=0.9',\n",
    "        'Referer': 'https://www.ecosia.org/'\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "# URLs que se van a scrapear, en cada una de filtra por venta, Córdoba y un tipo de situación de vivienda. Se hace así porque ese filtro no se puede sacar del propio anuncio\n",
    "urls_y_situaciones = {\n",
    "    'https://www.idealista.com/venta-viviendas/cordoba-cordoba/con-nuda-propiedad,inquilino,ocupado/': 'Nuda propiedad, Alquilada, Ocupada',\n",
    "    'https://www.idealista.com/venta-viviendas/cordoba-cordoba/con-estudios,de-un-dormitorio,sin-inquilinos/': 'Disponible',\n",
    "    'https://www.idealista.com/venta-viviendas/cordoba-cordoba/con-de-dos-dormitorios,sin-inquilinos/': 'Disponible',\n",
    "    'https://www.idealista.com/venta-viviendas/cordoba-cordoba/con-de-tres-dormitorios,sin-inquilinos/': 'Disponible',\n",
    "    'https://www.idealista.com/venta-viviendas/cordoba-cordoba/con-de-cuatro-cinco-habitaciones-o-mas,sin-inquilinos/': 'Disponible'\n",
    "}\n",
    "\n",
    "# Inicializamos la lista donde se guardarán los anuncios\n",
    "resultados = []\n",
    "\n",
    "# Los anuncios obtenidos se guardarán en fichero tipo csv y json, para posteriormente elegir el mejor formato\n",
    "\n",
    "# Abrimos archivo CSV para guardar los resultados estructurados, mode='w' porque es modo escritura y lo reescribe de 0, newline='' para evitar escribir lineas en blanco entre lineas de csv y codificación utf-8\n",
    "with open('idealista_resultados1.csv', mode='w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.writer(file)\n",
    "\n",
    "    # Nombres de cabecera del csv\n",
    "    writer.writerow([\n",
    "        'Título', 'Precio', 'Tipo de vivienda', 'Zona', 'Habitaciones', 'Metros cuadrados',\n",
    "        'Baños', 'Planta', 'Estado', 'Situación vivienda', 'Tipo de anuncio', 'Multimedia',\n",
    "        'Aire acondicionado', 'Armarios empotrados', 'Ascensor', 'Balcón o terraza',\n",
    "        'Exterior', 'Garaje', 'Jardín', 'Piscina', 'Trastero', 'Vivienda accesible',\n",
    "        'Vivienda de lujo', 'Descripción',\n",
    "        'Metros cabecera',\n",
    "        'Orientación', 'Construido en', 'Acceso movilidad', 'Calefacción',\n",
    "        'Consumo energético', 'Emisiones', 'Equipamiento extra'\n",
    "    ])\n",
    "    \n",
    "    # Recorremos cada una de las URLs con su tipo de situación\n",
    "    for base_list_url, situacion_fijada in urls_y_situaciones.items():\n",
    "\n",
    "\n",
    "        # Construimos la URL combinando situación y habitaciones\n",
    "        url_modificada = base_list_url\n",
    "        # Se imprime un texto en negrita del tipo de situación que se va a scrapear\n",
    "        print(f\"\\033[1m\\nSe comienza a scrapear: {situacion_fijada}\\033[0m\")\n",
    "        \n",
    "        # Inicializamos que estamos en la página 1 de ese tipo de situación, porque luego habrá que recorrer todas las páginas\n",
    "        pagina = 1\n",
    "        \n",
    "        while True:\n",
    "            if pagina == 1: # Si estamos en la página 1 cogemos la URL base que siempre lleva a la página 1\n",
    "                list_url = url_modificada \n",
    "            else:#Si no estamos en la 1 hay que modificar la URL para que busque en la página correspondiente\n",
    "                list_url = url_modificada.rstrip('/') + f'/pagina-{pagina}.htm' \n",
    "                \n",
    "            print(f\"\\nScrapeando el tipo de situación: {situacion_fijada} en la URL: {list_url}\")\n",
    "\n",
    "            #Función de la librería requests para hacer web scraping, eligiendo el header aleatoriamente\n",
    "            headers = random.choice(headers_list)\n",
    "            response = requests.get(list_url, headers=headers)\n",
    "            \n",
    "            #Si se obtiene un error al acceder a la página se sale del bucle        \n",
    "            if response.status_code != 200:\n",
    "                print(\"Se ha obtenido un error al acceder a la URL, error: \", response.status_code)\n",
    "                break\n",
    "\n",
    "            #Analizar código HTML para extraer información de él\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            listings = soup.find_all('article')\n",
    "\n",
    "            #Si no hay anuncios se sale del bucle\n",
    "            if not listings:\n",
    "                print(\"sin anuncios\")\n",
    "                break\n",
    "\n",
    "            #Se guarda la URL del primer anuncio que sale en la URL buscada    \n",
    "            first_listing = listings[0].find('a', class_='item-link')\n",
    "            url_anuncio_actual = first_listing['href'] if first_listing and first_listing.has_attr('href') else None\n",
    "\n",
    "            # Guardamos la URL del primer anuncio de la primera página, porque si una página no existe (por ejemplo, la página 99), el sitio web redirige automáticamente a la página 1.\n",
    "            # Comparando la URL del primer anuncio de cada página con la de la página 1, podemos saber si hemos llegado al final del scrapeo.\n",
    "            if pagina == 1:\n",
    "                url_anuncio_primera_pagina = url_anuncio_actual#solo se compara con el primer anuncio de la primera pagian, proque cuando la pagina no existe redirige a la primera\n",
    "            else:\n",
    "                if url_anuncio_actual == url_anuncio_primera_pagina:\n",
    "                    print(f\"La página no existe y se ha redirigido a la 1, scrapeo de la situación: {situacion_fijada} ha finalizado\")\n",
    "                    break\n",
    "\n",
    "            # Imprimimos el número de anuncios que hay en cada página\n",
    "            print(f\" Hay {len(listings)} anuncios en la página {pagina}\")\n",
    "\n",
    "            # Iteramos por todos los anuncios obtenidos\n",
    "            for listing in listings:\n",
    "\n",
    "                # Inicialización de los campos a extraer de cada anuncio\n",
    "                title = price = tipo_vivienda = situacion = habitaciones_anuncio = metros_anuncio = baños = planta_anuncio = estado = situacion_vivienda = tipo_anuncio = descripcion  = 'N/A'\n",
    "                metros_construidos = orientacion = construido_en = acceso_movilidad = calefaccion = consumo_energetico = emisiones_energeticas = 'N/A'\n",
    "                caracteristicas = equipamiento_extra = []\n",
    "                multimedia = \"Sin fotos\"\n",
    "                situacion_vivienda = situacion_fijada\n",
    "                aire_acondicionado = armarios = ascensor = balcon = exterior = garaje = jardin = piscina = trastero = accesible = lujo = 'No'\n",
    "\n",
    "                # Para cada anuncio se extrae: el título, el precio y el link\n",
    "                title_tag = listing.find('a', class_='item-link')\n",
    "                title = title_tag.get_text(strip=True) if title_tag else 'N/A'\n",
    "                if title == 'N/A':\n",
    "                    continue\n",
    "                price_tag = listing.find('span', class_='item-price')\n",
    "                if price_tag:\n",
    "                    price_text = price_tag.get_text(strip=True)\n",
    "                    match = re.search(r'[\\d.]+', price_text)\n",
    "                    if match:\n",
    "                        # Elimina puntos y convierte a número\n",
    "                        price = match.group().replace('.', '')\n",
    "                    else:\n",
    "                        price = 'N/A'\n",
    "                else:\n",
    "                    price = 'N/A'\n",
    "                link = base_url + title_tag['href'] if title_tag and title_tag.has_attr('href') else None\n",
    "                \n",
    "                # Si el enlace es válido, lanzamos una nueva consulta al HTML del anuncio\n",
    "                if link:\n",
    "                    anuncio_res = requests.get(link, headers=headers)\n",
    "                    if anuncio_res.status_code == 200:\n",
    "                        anuncio_soup = BeautifulSoup(anuncio_res.content, 'html.parser')\n",
    "\n",
    "                        title_tag = anuncio_soup.find('title')\n",
    "\n",
    "                        \n",
    "                        #Según los patrones que tenga el título obtenemos el tipo de vivienda\n",
    "                        if title_tag:\n",
    "                            title_text = title_tag.get_text(strip=True).lower()\n",
    "                            if \"piso\" in title_text:\n",
    "                                tipo_vivienda = \"Piso\"\n",
    "                            elif \"ático\" in title_text:\n",
    "                                tipo_vivienda = \"Ático\"\n",
    "                            elif \"dúplex\" in title_text or \"duplex\" in title_text:\n",
    "                                tipo_vivienda = \"Dúplex\"\n",
    "                            elif \"independiente\" in title_text:\n",
    "                                tipo_vivienda = \"Casa o chalet independiente\"\n",
    "                            elif \"pareado\" in title_text:\n",
    "                                tipo_vivienda = \"Chalet pareado\"\n",
    "                            elif \"adosado\" in title_text:\n",
    "                                tipo_vivienda = \"Chalet adosado\"\n",
    "                            elif \"rústica\" in title_text or \"cortijo\" in title_text or \"rural\" in title_text:\n",
    "                                tipo_vivienda = \"Casa rústica\"\n",
    "                            elif \"estudio\" in title_text:\n",
    "                                tipo_vivienda = \"Estudio\"\n",
    "\n",
    "                        # Se obtiene la localización\n",
    "                        location_tag = anuncio_soup.find('span', class_='main-info__title-minor')\n",
    "                        situacion = location_tag.get_text(strip=True) if location_tag else 'N/A'\n",
    "\n",
    "                        # De la cabecera del anuncio sacamos los metros cuadrados, el número de habitaciones y la planta\n",
    "                        cabecera = anuncio_soup.find_all('div', class_='info-features')\n",
    "                        if cabecera:\n",
    "                            cab_texts = cabecera[0].get_text(\" | \", strip=True).split(\" | \")\n",
    "                            for text in cab_texts:\n",
    "                                if \"m²\" in text:\n",
    "                                    match = re.search(r'\\d+', text)\n",
    "                                    metros_anuncio = match.group() if match else 'N/A'\n",
    "                                elif \"hab\" in text:\n",
    "                                    match = re.search(r'\\d+', text)\n",
    "                                    habitaciones_anuncio = match.group() if match else 'N/A'\n",
    "                                elif \"Planta\" in text:\n",
    "                                    planta_anuncio = text\n",
    "\n",
    "                        # Se obtiene la descripción\n",
    "                        desc_tag = anuncio_soup.find('div', {'class': 'adCommentsLanguage'})\n",
    "                        descripcion = desc_tag.get_text(strip=True) if desc_tag else 'Sin descripción'\n",
    "\n",
    "                        #Se obtienen el resto de caracteristicas del anuncio\n",
    "                        caracteristicas_raw = anuncio_soup.find_all('li')\n",
    "                        for li in caracteristicas_raw:\n",
    "                            texto = li.get_text(strip=True)\n",
    "                            caracteristicas.append(texto)\n",
    "\n",
    "                            if \"m² construidos\" in texto:\n",
    "                                metros_construidos = texto\n",
    "                            elif \"baño\" in texto:\n",
    "                                match = re.search(r'\\d+', texto)\n",
    "                                baños = match.group() if match else 'N/A'\n",
    "                            elif \"Planta\" in texto:\n",
    "                                planta = texto\n",
    "                            elif \"segunda mano\" in texto.lower() or \"nuevo\" in texto.lower():\n",
    "                                estado = texto\n",
    "                            elif \"orientación\" in texto.lower():\n",
    "                                orientacion = texto\n",
    "                            elif \"construido en\" in texto.lower():\n",
    "                                construido_en = texto\n",
    "                            elif \"movilidad reducida\" in texto.lower():\n",
    "                                acceso_movilidad = texto\n",
    "                            elif \"calefacción\" in texto.lower():\n",
    "                                calefaccion = texto\n",
    "                            elif \"consumo:\" in texto.lower():\n",
    "                                consumo_energetico = texto\n",
    "                            elif \"emisiones:\" in texto.lower():\n",
    "                                emisiones_energeticas = texto\n",
    "                            elif \"aire acondicionado\" in texto.lower():\n",
    "                                aire_acondicionado = 'Sí'\n",
    "                            elif \"armarios empotrados\" in texto.lower():\n",
    "                                armarios = 'Sí'\n",
    "                            elif \"ascensor\" in texto.lower():\n",
    "                                ascensor = 'Sí'\n",
    "                            elif \"balcón\" in texto or \"terraza\" in texto.lower():\n",
    "                                balcon = 'Sí'\n",
    "                            elif \"exterior\" in texto.lower():\n",
    "                                exterior = 'Sí'\n",
    "                            elif \"garaje\" in texto.lower():\n",
    "                                garaje = 'Sí'\n",
    "                            elif \"jardín\" in texto.lower():\n",
    "                                jardin = 'Sí'\n",
    "                            elif \"piscina\" in texto.lower():\n",
    "                                piscina = 'Sí'\n",
    "                            elif \"trastero\" in texto.lower():\n",
    "                                trastero = 'Sí'\n",
    "                            elif \"accesible\" in texto.lower():\n",
    "                                accesible = 'Sí'\n",
    "                            elif \"lujo\" in texto.lower():\n",
    "                                lujo = 'Sí'\n",
    "\n",
    "                        # Recorre todos los <div> del anuncio buscando un encabezado (<h2> o <h3>) que contenga la palabra “equipamiento”, dentro busca una lista <ul> y dentro los <li>\n",
    "                        for div in anuncio_soup.find_all('div'):\n",
    "                            heading = div.find(['h2', 'h3'])\n",
    "                            if heading and 'equipamiento' in heading.get_text(strip=True).lower():\n",
    "                                ul = div.find('ul')\n",
    "                                if ul:\n",
    "                                    for li in ul.find_all('li'):\n",
    "                                        texto = li.get_text(strip=True)\n",
    "                                        if texto and texto not in equipamiento_extra:\n",
    "                                            equipamiento_extra.append(texto)\n",
    "                                break\n",
    "\n",
    "                        # Bloque para anlizar si el anuncio tiene imágenes\n",
    "                        imagenes = anuncio_soup.find_all('img', src=True)\n",
    "                        fotos_reales = [\n",
    "                            img['src'] for img in imagenes\n",
    "                            if any(palabra in img['src'] for palabra in ['image.master', 'img4.idealista.com', 'foto'])\n",
    "                        ]\n",
    "                        multimedia = \"Con fotos\" if fotos_reales else \"Sin fotos\"\n",
    "\n",
    "                        # Para saber si es de un particular o profesional\n",
    "                        tipo_tag = anuncio_soup.find('div', class_='professional-name')\n",
    "                        tipo_anuncio = 'Profesional' if tipo_tag else 'Particular'\n",
    "\n",
    "                    #Espera de 1 segundo entre cada petición al servidor de Idealista. Para no parecer un robot y evitar ser bloqueado.\n",
    "                    time.sleep(random.uniform(20,40))\n",
    "\n",
    "                # Guarda los datos de cada anuncio en el csv\n",
    "                writer.writerow([\n",
    "                    title, price, tipo_vivienda, situacion, habitaciones_anuncio, metros_anuncio,\n",
    "                    baños, planta_anuncio, estado, situacion_vivienda, tipo_anuncio,\n",
    "                    multimedia, aire_acondicionado, armarios, ascensor, balcon, exterior,\n",
    "                    garaje, jardin, piscina, trastero, accesible, lujo, descripcion,\n",
    "                    metros_construidos, orientacion, construido_en, acceso_movilidad,\n",
    "                    calefaccion, consumo_energetico, emisiones_energeticas, \", \".join(equipamiento_extra)\n",
    "                ])\n",
    "\n",
    "                # Guardar los datos de cada anuncio en el json\n",
    "                resultados.append({\n",
    "                    \"Título\": title, \"Precio\": price, \"Tipo de vivienda\": tipo_vivienda, \"Zona\": situacion,\n",
    "                    \"Habitaciones\": habitaciones_anuncio, \"Metros cuadrados\": metros_anuncio, \"Baños\": baños, \"Planta\": planta_anuncio,\n",
    "                    \"Estado\": estado, \"Situación vivienda\": situacion_vivienda, \"Tipo de anuncio\": tipo_anuncio, \"Multimedia\": multimedia,\n",
    "                    \"Aire acondicionado\": aire_acondicionado, \"Armarios empotrados\": armarios,\n",
    "                    \"Ascensor\": ascensor, \"Balcón o terraza\": balcon, \"Exterior\": exterior,\n",
    "                    \"Garaje\": garaje, \"Jardín\": jardin, \"Piscina\": piscina, \"Trastero\": trastero,\n",
    "                    \"Vivienda accesible\": accesible, \"Vivienda de lujo\": lujo, \"Descripción\": descripcion,\n",
    "                    \"Metros construidos/utiles\": metros_construidos,\n",
    "                    \"Orientación\": orientacion, \"Construido en\": construido_en,\n",
    "                    \"Acceso movilidad\": acceso_movilidad, \"Calefacción\": calefaccion,\n",
    "                    \"Consumo energético\": consumo_energetico, \"Emisiones\": emisiones_energeticas,\n",
    "                    \"Equipamiento extra\": equipamiento_extra\n",
    "                })\n",
    "\n",
    "            # Se suma 1 al número de páginas para visitarlas todas\n",
    "            time.sleep(random.uniform(200, 300))\n",
    "            pagina += 1\n",
    "\n",
    "# Crea el json para guardar todos los resultados\n",
    "with open('idealista_resultados1.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(resultados, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "#Insertar datos en MongoDB\n",
    "\n",
    "# Conexión local, cambia el URI si tu MongoDB está en Atlas u otro lugar\n",
    "client = MongoClient('mongodb://localhost:27017/')\n",
    "db = client['Idealista']\n",
    "collection = db['Anuncios_Cordoba']\n",
    "\n",
    "#Borrar la colección existente\n",
    "collection.delete_many({})\n",
    "\n",
    "# Inserta todos los resultados en la colección\n",
    "collection.insert_many(resultados)\n",
    "\n",
    "# Mensaje avisando que se han guardado todos los resultados\n",
    "print(\"\\n Datos guardados en idealista_resultados1.csv y idealista_resultados1.json\")\n",
    "\n",
    "#Tiempo al finalizar de ejecutar el código: para saber el tiempo total de ejecucción\n",
    "fin = time.time()\n",
    "duracion = fin - inicio\n",
    "\n",
    "# Mensaje para saber el tiempo total que tarda en scrapear la página\n",
    "print(f\"\\n Tiempo total de ejecución: {duracion:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11873a4e-9fa7-4fee-b957-f394b533ac9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
